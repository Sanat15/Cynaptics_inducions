{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow-datasets\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpu in gpus:\n",
    "    print(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(ds.as_numpy_iterator())\n",
    "print(first_batch.keys())  # Should output: dict_keys(['image'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataiterator = ds.as_numpy_iterator() # set up a connection to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(dataiterator.next()['image']).shape  # get the next batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Path to your custom dataset\n",
    "CUSTOM_DATASET_DIR = \"C:/Users/HP/Documents/Programs/Cynaptics/Data/Train/Real\"\n",
    "\n",
    "# Function to preprocess and load images\n",
    "def load_and_preprocess_image(image_path, target_size=(28, 28)):\n",
    "    image_path = image_path.numpy().decode('utf-8')  # Convert TensorFlow tensor to Python string\n",
    "    image = load_img(image_path, target_size=target_size, color_mode=\"grayscale\")  # Fashion MNIST is grayscale\n",
    "    image = img_to_array(image) / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Wrapper to handle `numpy()` conversion in the map function\n",
    "def load_and_preprocess_image_wrapper(image_path, target_size=(28, 28)):\n",
    "    return tf.py_function(func=load_and_preprocess_image, inp=[image_path], Tout=tf.float32)\n",
    "\n",
    "# Function to create a TensorFlow Dataset\n",
    "def create_custom_dataset(directory, target_size=(28, 28), batch_size=32):\n",
    "    image_paths = [os.path.join(directory, fname) for fname in os.listdir(directory) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    labels = [0] * len(image_paths)  # Dummy labels, as the dataset has no explicit labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: {'image': load_and_preprocess_image_wrapper(x, target_size), 'label': y}, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=len(image_paths)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create the custom dataset\n",
    "ds = create_custom_dataset(CUSTOM_DATASET_DIR)\n",
    "\n",
    "# Visualization of Images with Matplotlib\n",
    "dataiterator = iter(ds)  # Create an iterator for the dataset\n",
    "\n",
    "# Set up the sub-plot format\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))  # fig is the whole plot, ax is each subplot\n",
    "for idx in range(4):\n",
    "    # Grab an image and label\n",
    "    batch = next(dataiterator)  # A sample\n",
    "    image = np.squeeze(batch['image'][idx].numpy())  # Extract the image and remove batch dimensions\n",
    "    label = batch['label'][idx].numpy()  # Extract the corresponding label\n",
    "    # Plot the image using a specific axis\n",
    "    ax[idx].imshow(image, cmap='gray')  # Display the grayscale image\n",
    "    ax[idx].set_title(f\"Label: {label}\")  # Append image label as plot title\n",
    "    ax[idx].axis('off')  # Hide axes for clarity\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing , scaling images to 0-1\n",
    "def scale_images(data):\n",
    "  image = data['image']\n",
    "  return image/ 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ds = tfds.load('fashion_mnist', split='train') # Reloaded dataset , optional\n",
    "ds = ds.map(scale_images) # Running dataset through the scaling function\n",
    "ds = ds.cache() # Caching the dataset for that batch\n",
    "ds = ds.shuffle(60000) #Shuffling the dataset\n",
    "ds = ds.batch(128) # Dividing the dataset into batches of 128\n",
    "ds = ds.prefetch(64)  # Eliminates bottelneckind by prefetching the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing in sqquential api for generator and discriminator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Reshape , Flatten, Conv2D, Dropout,UpSampling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(): \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Takes in random values and reshapes it to 7x7x128\n",
    "    # Beginnings of a generated image\n",
    "    model.add(Dense(7*7*128, input_dim=128))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Reshape((7,7,128)))\n",
    "    \n",
    "    # Upsampling block 1 \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 5, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Upsampling block 2 \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 5, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Convolutional block 1\n",
    "    model.add(Conv2D(128, 4, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Convolutional block 2\n",
    "    model.add(Conv2D(128, 4, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Conv layer to get to one channel\n",
    "    model.add(Conv2D(1, 4, padding='same', activation='sigmoid'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the generator\n",
    "img = generator.predict(np.random.randn(4,128,1))\n",
    "# Set up the sub-plot format\n",
    "\n",
    "fig ,ax = plt.subplots(ncols = 4,figsize=(20,20)) # fig is whole plot , ax is each subplot\n",
    "for idx , img in enumerate(img):\n",
    "\n",
    "  #Plot the image using a spicific axis\n",
    "  ax[idx].imshow(np.squeeze(img)) # plot using a specific subplot\n",
    "  ax[idx].set_title(idx) #Appending image lable as plot title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "  model = Sequential()\n",
    "  \n",
    "  #First Conv block\n",
    "  model.add(Conv2D(32 , 5, input_shape=(28,28,1)))\n",
    "  model.add(LeakyReLU(0.2))\n",
    "  model.add(Dropout(0.4))\n",
    "  \n",
    "  #Second Conv block\n",
    "  model.add(Conv2D(64 , 5))\n",
    "  model.add(LeakyReLU(0.2))\n",
    "  model.add(Dropout(0.4))\n",
    "  \n",
    "  #Third Conv block\n",
    "  model.add(Conv2D(128 , 5))\n",
    "  model.add(LeakyReLU(0.2))\n",
    "  model.add(Dropout(0.4))\n",
    "  \n",
    "  #Forth Conv block\n",
    "  model.add(Conv2D(256 , 5))\n",
    "  model.add(LeakyReLU(0.2))\n",
    "  model.add(Dropout(0.4))\n",
    "  \n",
    "  #Flatten then pass through dense layer\n",
    "  model.add(Flatten())\n",
    "  model.add(Dropout(0.4))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.predict(np.expand_dims(img,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the optimizer and loss function\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_opt = Adam(learning_rate=0.0001)\n",
    "d_opt = Adam(learning_rate=0.00001)\n",
    "g_loss = BinaryCrossentropy()\n",
    "d_loss = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mporting base model class into sub-class\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(Model): \n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        # Pass through args and kwargs to base class \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for gen and disc\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator \n",
    "        \n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for losses and optimizers\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss \n",
    "\n",
    "    def train_step(self, batch):\n",
    "        # Get the data \n",
    "        real_images = batch\n",
    "        fake_images = self.generator(tf.random.normal((128, 128, 1)), training=False)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            # Pass the real and fake images to the discriminator model\n",
    "            yhat_real = self.discriminator(real_images, training=True) \n",
    "            yhat_fake = self.discriminator(fake_images, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "            \n",
    "            # Create labels for real and fakes images\n",
    "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
    "            \n",
    "            # Add some noise to the TRUE outputs\n",
    "            noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n",
    "            noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n",
    "            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n",
    "            \n",
    "            # Calculate loss - BINARYCROSS \n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "            \n",
    "        # Apply backpropagation - nn learn \n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator \n",
    "        with tf.GradientTape() as g_tape: \n",
    "            # Generate some new images\n",
    "            gen_images = self.generator(tf.random.normal((128,128,1)), training=True)\n",
    "                                        \n",
    "            # Create the predicted labels\n",
    "            predicted_labels = self.discriminator(gen_images, training=False)\n",
    "                                        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n",
    "            \n",
    "        # Apply backprop\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create instance of the subclass model\n",
    "gan = GAN(generator , discriminator)\n",
    "#Compile the model\n",
    "gan.compile(g_opt , d_opt , g_loss , d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitor(Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim,1))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = array_to_img(generated_images[i])\n",
    "            img.save(os.path.join('generated_images', f'generated_img_{epoch}_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = gan.fit(ds , epochs = 2000 ,callbacks = [ModelMonitor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Loss')\n",
    "plt.plot(hist.history['d_loss'], label='Discriminator Loss')\n",
    "plt.plot(hist.history['g_loss'], label='Generator Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = generator(tf.random.normal((16 , 128 , 1)), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(ncols = 4,nrows =4, figsize=(20,20))\n",
    "for idx in range(4):\n",
    "  for c in range(4):\n",
    "    ax[idx][c].imshow(imgs[(idx+1)*(c+1)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class HangmanGuessModel(nn.Module):\n",
    "    def __init__(self, input_size_word=30, input_size_letters=26, hidden_size=128, output_size=26):\n",
    "        super(HangmanGuessModel, self).__init__()\n",
    "\n",
    "        # Embedding for words (letters + special tokens)\n",
    "        self.word_embedding = nn.Embedding(28, hidden_size)  # 26 letters + '_' + space\n",
    "        self.guessed_fc = nn.Linear(input_size_letters, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)  # LSTM for sequential data\n",
    "        self.fc_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.output_fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def create_guess_tensors(self, guessed_letters):\n",
    "        tensor = torch.zeros((1, 26))\n",
    "        for letter in guessed_letters:\n",
    "            index = ord(letter) - ord('a')\n",
    "            if 0 <= index < 26:\n",
    "                tensor[0, index] = 1\n",
    "        return tensor\n",
    "\n",
    "    def preprocess_word(self, word):\n",
    "        indices = []\n",
    "        for letter in word.replace(\" \", \"\"):\n",
    "            if letter == '_':\n",
    "                indices.append(27)\n",
    "            else:\n",
    "                indices.append(ord(letter) - ord('a') + 1)\n",
    "        return torch.tensor(indices, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, masked_word, guessed_letters):\n",
    "        word_indices = self.preprocess_word(masked_word)\n",
    "        guessed_tensor = self.create_guess_tensors(guessed_letters)\n",
    "\n",
    "        # Pass word through embedding and LSTM\n",
    "        word_features = self.word_embedding(word_indices)\n",
    "        lstm_out, _ = self.lstm(word_features)\n",
    "        word_features = lstm_out[:, -1, :]  # Take the last LSTM output\n",
    "\n",
    "        # Process guessed letters\n",
    "        guessed_features = F.relu(self.guessed_fc(guessed_tensor))\n",
    "\n",
    "        # Combine and pass through the final layers\n",
    "        combined_features = torch.cat((word_features, guessed_features), dim=1)\n",
    "        combined_features = F.relu(self.fc_combine(combined_features))\n",
    "        return self.output_fc(combined_features)\n",
    "\n",
    "\n",
    "class CynapticsHangman:\n",
    "    def __init__(self):\n",
    "        self.guessed_letters = []\n",
    "        self.lives_remaining = 6\n",
    "        self.train_file = \"train.txt\"\n",
    "        self.valid_file = \"valid.txt\"\n",
    "        self.train_dict = self.build_dictionary(self.train_file)\n",
    "        self.valid_dict = self.build_dictionary(self.valid_file)\n",
    "        self.device = self.get_device()\n",
    "        self.model = HangmanGuessModel().to(self.device)\n",
    "\n",
    "    def get_device(self):\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def build_dictionary(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            lines = file.read().splitlines()\n",
    "            if not lines:\n",
    "                raise ValueError(f\"The dictionary file at {file_path} is empty.\")\n",
    "            return lines\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "    def generate_masked_variants(self, word):\n",
    "        variants = []\n",
    "        for _ in range(5):\n",
    "            mask = ''.join('_' if random.random() < 0.5 else letter for letter in word)\n",
    "            variants.append(mask)\n",
    "        return variants\n",
    "\n",
    "    def guess(self, masked_word, lives_left):\n",
    "        probabilities = self.model(masked_word, self.guessed_letters)\n",
    "        \n",
    "        # Create a mask to exclude already guessed letters\n",
    "        mask = torch.ones_like(probabilities)\n",
    "        for letter in self.guessed_letters:\n",
    "            index = ord(letter) - ord('a')\n",
    "            if 0 <= index < 26:\n",
    "                mask[0, index] = 0\n",
    "\n",
    "        masked_probabilities = probabilities * mask\n",
    "\n",
    "        # If no valid guess is available, randomly choose an unguessed letter\n",
    "        if torch.max(masked_probabilities) == 0:\n",
    "            remaining_letters = [\n",
    "                chr(i + ord('a'))\n",
    "                for i in range(26)\n",
    "                if chr(i + ord('a')) not in self.guessed_letters\n",
    "            ]\n",
    "            guessed_letter = random.choice(remaining_letters)\n",
    "        else:\n",
    "            predicted_index = torch.argmax(masked_probabilities).item()\n",
    "            guessed_letter = chr(predicted_index + ord('a'))\n",
    "\n",
    "        return guessed_letter\n",
    "\n",
    "    def return_status(self, word, masked_word, guessed_letter):\n",
    "        if guessed_letter in word:\n",
    "            masked_word = ''.join([c if c in self.guessed_letters + [guessed_letter] else '_' for c in word])\n",
    "            if '_' in masked_word:\n",
    "                return \"ongoing\", \"Correct guess\", masked_word\n",
    "            return \"success\", \"Word guessed\", masked_word\n",
    "        self.lives_remaining -= 1\n",
    "        if self.lives_remaining == 0:\n",
    "            return \"failed\", \"No lives left\", masked_word\n",
    "        return \"ongoing\", \"Wrong guess\", masked_word\n",
    "\n",
    "    def start_game(self, game_id, verbose=True):\n",
    "        self.guessed_letters = [' ']\n",
    "        word = random.choice(self.valid_dict)\n",
    "        masked_word = '_' * len(word)\n",
    "        self.lives_remaining = 6\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Game {game_id} started. Word: {' '.join(masked_word)}\")\n",
    "\n",
    "        while self.lives_remaining > 0:\n",
    "            guess = self.guess(masked_word, self.lives_remaining)\n",
    "            self.guessed_letters.append(guess)\n",
    "\n",
    "            status, message, masked_word = self.return_status(word, masked_word, guess)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Guess: {guess}, {message}. Masked: {masked_word}\")\n",
    "\n",
    "            if status == \"success\":\n",
    "                if verbose:\n",
    "                    print(f\"Game {game_id} won! Word: {word}\")\n",
    "                return True\n",
    "\n",
    "            if status == \"failed\":\n",
    "                if verbose:\n",
    "                    print(f\"Game {game_id} lost! Word: {word}\")\n",
    "                return False\n",
    "\n",
    "        return False\n",
    "\n",
    "    def train(self, episodes=100):\n",
    "        print(\"Starting training...\")\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "        training_loss = []\n",
    "        for episode in range(episodes):\n",
    "            try:\n",
    "                word = random.choice(self.train_dict)\n",
    "                masked_variants = self.generate_masked_variants(word)\n",
    "\n",
    "                for masked_word in masked_variants:\n",
    "                    self.guessed_letters = []\n",
    "                    self.lives_remaining = 6\n",
    "                    max_tries = len(word) + 6\n",
    "\n",
    "                    while '_' in masked_word and self.lives_remaining > 0 and max_tries > 0:\n",
    "                        max_tries -= 1\n",
    "                        guess = self.guess(masked_word, self.lives_remaining)\n",
    "                        self.guessed_letters.append(guess)\n",
    "\n",
    "                        probabilities = self.model(masked_word, self.guessed_letters)\n",
    "                        target = torch.zeros((1, 26)).to(self.device)\n",
    "                        for letter in word:\n",
    "                            if letter not in self.guessed_letters:\n",
    "                                target[0, ord(letter) - ord('a')] = 1\n",
    "\n",
    "                        loss = criterion(probabilities, target)\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    training_loss.append(loss.item())\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                if (episode + 1) % 10 == 0:\n",
    "                    avg_loss = sum(training_loss[-10:]) / 10\n",
    "                    print(f\"Episode {episode + 1}/{episodes}: Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in episode {episode + 1}: {e}\")\n",
    "\n",
    "        self.save_model(\"improved_hangman_model.pth\")\n",
    "        print(\"Training complete.\")\n",
    "        \n",
    "    def summary(self):\n",
    "        \"\"\"Print a summary of the Hangman model and its parameters\"\"\"\n",
    "        print(\"CynapticsHangman Model Summary\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Model Architecture: {self.model.__class__.__name__}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Number of Lives: {self.lives_remaining}\")\n",
    "        print(f\"Dictionary Size: {len(self.valid_dict)}\")\n",
    "        print(f\"Guessed Letters: {', '.join(self.guessed_letters)}\")\n",
    "        \n",
    "        # Print model parameters\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "        print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Print model structure\n",
    "        print(\"\\nModel Structure:\")\n",
    "        print(self.model)\n",
    "\n",
    "\n",
    "# # Example Usage\n",
    "hangman = CynapticsHangman()\n",
    "# hangman.train(episodes=100)\n",
    "# hangman.load_model(\"improved_hangman_model.pth\")\n",
    "\n",
    "# win_count = 0\n",
    "# total_games = 10\n",
    "# for i in range(total_games):\n",
    "#     if hangman.start_game(i, verbose=True):\n",
    "#         win_count += 1\n",
    "\n",
    "# print(f\"Success Rate: {win_count / total_games:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(self):\n",
    "    \"\"\"Print a summary of the Hangman model and its parameters\"\"\"\n",
    "    print(\"CynapticsHangman Model Summary\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Model Architecture: {self.model.__class__.__name__}\")\n",
    "    print(f\"Device: {self.device}\")\n",
    "    print(f\"Number of Lives: {self.lives_remaining}\")\n",
    "    print(f\"Dictionary Size: {len(self.valid_dict)}\")\n",
    "    print(f\"Guessed Letters: {', '.join(self.guessed_letters)}\")\n",
    "    \n",
    "    # Print model parameters\n",
    "    total_params = sum(p.numel() for p in self.model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Print model structure\n",
    "    print(\"\\nModel Structure:\")\n",
    "    print(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CynapticsHangman Model Summary\n",
      "----------------------------------------\n",
      "Model Architecture: HangmanGuessModel\n",
      "Device: cpu\n",
      "Number of Lives: 6\n",
      "Dictionary Size: 22730\n",
      "Guessed Letters: \n",
      "\n",
      "Total Parameters: 175,386\n",
      "Trainable Parameters: 175,386\n",
      "\n",
      "Model Structure:\n",
      "HangmanGuessModel(\n",
      "  (word_embedding): Embedding(28, 128)\n",
      "  (guessed_fc): Linear(in_features=26, out_features=128, bias=True)\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (fc_combine): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output_fc): Linear(in_features=128, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hangman.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
